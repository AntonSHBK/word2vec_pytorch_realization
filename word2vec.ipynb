{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean(inp: str) -> str:\n",
    "\tinp = inp.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
    "\tinp = re.sub(r'\\s+', ' ', inp.lower())\n",
    "\treturn inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Epoch 1, Loss: 1173.5701942443848\n",
      "Epoch 2, Loss: 1095.9758143424988\n",
      "Epoch 3, Loss: 1048.6943447589874\n",
      "Epoch 4, Loss: 1016.8696548938751\n",
      "Epoch 5, Loss: 993.9360184669495\n",
      "{'handles': array([ 2.1932738 ,  2.0008218 ,  0.92650944, -2.5462065 , -0.45673686,\n",
      "       -0.604371  ,  0.0327921 ,  0.1279636 , -0.13157755, -0.8656726 ],\n",
      "      dtype=float32), 'vast': array([ 0.85359716, -0.83292633,  0.40998635, -2.0948365 , -0.66972643,\n",
      "        0.82953256,  1.3128676 ,  0.251736  , -2.0371852 ,  1.8580576 ],\n",
      "      dtype=float32), 'efficiently': array([ 0.16414103,  0.8674698 , -1.4432902 ,  1.3898158 ,  0.24523647,\n",
      "        0.42413908, -0.32576472, -0.83819926, -0.39766714,  0.17251293],\n",
      "      dtype=float32), 'semantic': array([-0.69772106,  0.92769605,  0.33417654, -0.56730735,  0.92007875,\n",
      "        0.46032202,  0.9568542 ,  0.58214265,  0.177861  ,  0.05878519],\n",
      "      dtype=float32), 'well': array([ 0.11023446, -0.38757363, -0.43595487,  0.32187295,  1.5768405 ,\n",
      "        0.38509765, -0.6025726 ,  1.0642335 ,  0.82544684,  0.92062604],\n",
      "      dtype=float32), 'to': array([-3.049988  ,  1.5560247 ,  0.46694076, -0.1574499 ,  1.0467905 ,\n",
      "       -0.50675935, -0.8553752 ,  0.01132375,  1.1047916 ,  0.17218149],\n",
      "      dtype=float32), 'its': array([ 0.23101968,  0.61144274, -1.1588672 , -0.71926075,  1.2196594 ,\n",
      "       -0.24019855, -0.4641348 , -0.16625726,  0.35127833, -0.90451324],\n",
      "      dtype=float32), 'effectively': array([-0.64253646, -1.5353836 ,  1.0564662 ,  0.34214967, -0.02546668,\n",
      "       -0.93399054, -1.1747056 ,  0.65072596, -0.33257785, -1.0228134 ],\n",
      "      dtype=float32), 'amounts': array([-4.11032677e-01,  2.81675041e-01, -1.07595071e-01,  7.98596740e-01,\n",
      "       -1.86417413e+00,  1.88779041e-01, -1.07041106e-01, -5.99108338e-01,\n",
      "       -9.41121459e-01,  4.75217588e-04], dtype=float32), 'encode': array([-0.06355629,  1.0063413 ,  0.68455374,  0.20951515,  0.3143543 ,\n",
      "        0.44720352,  0.04150704,  1.0310128 , -2.3908174 , -0.8382627 ],\n",
      "      dtype=float32), 'analogies': array([-0.12855211, -0.62210196,  1.2160757 , -0.45811465, -0.9294619 ,\n",
      "        1.2590672 ,  0.03881645,  0.38515434, -0.9390322 , -1.0583333 ],\n",
      "      dtype=float32), 'the': array([ 0.7975634 ,  0.9939608 , -1.4246941 ,  0.5589788 ,  0.24038747,\n",
      "        0.32152086,  1.7523353 ,  0.11423818,  1.3462387 , -0.12486811],\n",
      "      dtype=float32), 'potential': array([ 0.8684459 , -1.278256  , -1.2987858 , -0.65244514,  2.5566175 ,\n",
      "        0.38262656, -0.03724005, -0.35652912,  0.8865587 , -0.47845808],\n",
      "      dtype=float32), 'tasks': array([ 0.8150775 ,  1.1741807 ,  1.2062755 ,  1.9261138 ,  1.4587628 ,\n",
      "       -0.43661976, -1.1110973 ,  1.7303753 , -0.8298545 , -0.41771427],\n",
      "      dtype=float32), 'limited': array([-0.8123809 , -1.451941  ,  0.89007396,  2.1762884 , -0.2619825 ,\n",
      "       -0.48422492, -0.17503431,  2.134736  ,  1.0683258 , -0.28490278],\n",
      "      dtype=float32), 'relationships': array([-0.29079247, -0.15102279,  0.65050125,  0.17120127, -0.7792363 ,\n",
      "       -0.79758275, -0.1383767 , -2.4904804 , -0.3159392 , -0.2804269 ],\n",
      "      dtype=float32), 'in': array([ 0.17706487,  1.0515149 , -2.0069041 ,  0.0097527 , -0.11653782,\n",
      "       -0.16900827, -1.2261115 , -0.8282204 ,  1.554011  ,  1.7490696 ],\n",
      "      dtype=float32), 'that': array([ 1.8171704 , -1.4284807 ,  0.93519044,  1.8467329 , -0.18300763,\n",
      "        0.37951314, -0.2304608 , -1.6938167 ,  1.251639  , -0.14224273],\n",
      "      dtype=float32), 'on': array([ 1.7887197 , -0.6417743 , -1.2230674 , -0.9614723 , -0.5143291 ,\n",
      "        0.72659045, -1.6647365 ,  2.380831  ,  0.27148733,  1.8415622 ],\n",
      "      dtype=float32), 'flexible': array([-1.2508274 , -1.7169344 ,  0.4088558 ,  0.7975807 , -1.6991609 ,\n",
      "        0.3489942 , -0.83620214,  0.5518091 ,  1.1071708 , -1.2782824 ],\n",
      "      dtype=float32), 'which': array([ 0.300273  ,  0.9742021 ,  0.06727083, -0.2372598 , -0.9711146 ,\n",
      "        0.31461602,  0.7371439 , -0.02889506,  1.2570122 ,  1.1050308 ],\n",
      "      dtype=float32), 'generate': array([ 2.622621  , -1.6665683 ,  0.49999538, -0.32432568, -0.1648105 ,\n",
      "        1.3920661 , -0.12474845,  0.47898516,  1.3728195 ,  0.45792443],\n",
      "      dtype=float32), 'like': array([-0.01292338,  0.7544963 ,  0.33581513,  0.8397661 , -1.1586723 ,\n",
      "       -0.12002494,  0.30127138, -1.1764479 , -0.29567894,  1.1854749 ],\n",
      "      dtype=float32), 'by': array([-0.05634876, -0.27059335,  1.6495135 ,  0.7390586 , -0.6266431 ,\n",
      "       -0.8735573 , -0.33320993, -0.2647523 , -0.94522536, -1.070272  ],\n",
      "      dtype=float32), 'or': array([-1.7607849 , -0.33416492, -1.9685129 , -2.1360638 , -1.2437764 ,\n",
      "       -0.56655055,  1.4884857 , -1.4632365 ,  0.62624973,  0.32521313],\n",
      "      dtype=float32), 'data': array([-0.38352793, -1.4663674 , -1.1266341 , -0.2773622 ,  0.11445574,\n",
      "        1.7345076 , -0.66793257,  0.18981445, -1.5211543 ,  0.2783308 ],\n",
      "      dtype=float32), 'around': array([ 0.9956611 ,  0.42599836, -0.39324385,  0.08564434, -0.57949966,\n",
      "        0.54137826,  1.4457805 , -0.20603573, -0.11013155, -1.930856  ],\n",
      "      dtype=float32), 'for': array([ 1.9146556 ,  0.6345359 , -1.5720145 ,  0.04597846,  1.0093247 ,\n",
      "        1.0194023 , -0.64565116, -0.20965286, -1.0422786 ,  1.3285632 ],\n",
      "      dtype=float32), 'similar': array([-1.1462226 ,  0.7258091 , -0.5920304 ,  0.03516237,  0.12734969,\n",
      "       -0.26521856, -1.0948896 ,  0.30777007, -0.4065462 , -0.3314778 ],\n",
      "      dtype=float32), 'target': array([-0.971985  , -1.0159984 ,  1.4539233 ,  0.36630756, -2.6995156 ,\n",
      "        1.2508109 , -0.7259043 , -0.5651326 ,  0.72710216,  0.10703235],\n",
      "      dtype=float32), 'leveraging': array([ 0.1736761 , -1.0664489 , -0.12987803, -0.17186147, -0.01329687,\n",
      "        0.6962547 ,  0.38269773, -0.5262637 , -0.57416576,  0.3991093 ],\n",
      "      dtype=float32), 'of': array([-1.3436736 , -0.16335517,  0.14439301,  0.41189486,  0.25171548,\n",
      "       -0.3553518 ,  1.1926315 ,  0.05983164, -0.12046948, -0.5647693 ],\n",
      "      dtype=float32), 'global': array([ 0.47306517, -1.7208635 , -0.27913687, -0.8450331 ,  0.05887933,\n",
      "        2.0068274 ,  1.8208401 , -0.41907248,  0.43352833, -2.0124269 ],\n",
      "      dtype=float32), 'definitions': array([ 1.0187356 ,  2.626529  , -0.9770875 , -0.88088113,  0.06827707,\n",
      "        0.77214235,  0.4057528 , -1.5894699 , -2.2028213 , -0.4773839 ],\n",
      "      dtype=float32), 'between': array([-0.6127825 ,  1.6620439 ,  0.07676394, -1.1219821 , -0.15485883,\n",
      "        0.0049241 , -0.43561774,  0.5920871 ,  0.71338385, -0.55555344],\n",
      "      dtype=float32), 'due': array([ 0.5425703 ,  0.32250297, -0.4594114 ,  0.48007065,  0.7492827 ,\n",
      "        1.9815289 , -1.3806562 ,  0.95724666,  2.3162515 , -0.36109054],\n",
      "      dtype=float32), 'word': array([ 1.5664318 , -0.18064629,  1.0217319 , -0.35476416, -0.26940978,\n",
      "       -0.97012556, -1.0149935 ,  1.7909772 ,  0.23169309, -0.05388563],\n",
      "      dtype=float32), 'similarity': array([ 0.6520417 , -0.27509284, -0.13888526, -1.8540001 , -0.29810268,\n",
      "        0.09574963, -0.57980144, -0.24143863, -0.4428856 ,  3.046546  ],\n",
      "      dtype=float32), 'text': array([-0.28344345, -0.2604782 , -0.45454717,  1.26935   ,  0.14133918,\n",
      "        0.73747665, -0.4609517 ,  0.761117  ,  0.22054401,  1.0511807 ],\n",
      "      dtype=float32), 'training': array([-0.20146629,  0.25114638, -0.8862015 , -0.373075  , -3.423543  ,\n",
      "        0.0509215 ,  0.10927399,  2.2129407 , -2.7200856 ,  1.1326227 ],\n",
      "      dtype=float32), 'using': array([-1.4191234 ,  1.3071679 ,  0.57117647, -0.30322164,  0.7005173 ,\n",
      "       -0.8305782 ,  1.5196842 , -0.00813714,  1.8194114 ,  0.79081976],\n",
      "      dtype=float32), 'largescale': array([-1.6343209 , -0.13538031,  0.1202035 ,  0.35929164, -0.06616732,\n",
      "       -0.09583529,  0.70734656, -0.53742224,  0.53266203,  0.03230054],\n",
      "      dtype=float32), 'richer': array([ 0.5859853 ,  0.79457563, -0.1618287 ,  0.5823168 ,  0.652026  ,\n",
      "        0.7626259 ,  0.4117968 ,  0.55492723,  0.7968408 , -0.88375306],\n",
      "      dtype=float32), 'embeddings': array([ 0.8268881 , -0.18716122,  1.8789381 , -0.1632393 ,  1.1747215 ,\n",
      "       -0.6346676 , -0.8889213 ,  0.5370506 ,  0.5345328 , -2.0394614 ],\n",
      "      dtype=float32), 'context': array([ 0.5637296 , -0.64636236,  0.32447556,  1.3528768 ,  0.03150576,\n",
      "       -0.72187734, -0.8432472 , -0.8769869 , -1.0739644 , -0.09921883],\n",
      "      dtype=float32), 'they': array([-0.57961774,  0.28606412, -0.6423218 , -0.4118281 ,  0.8507998 ,\n",
      "        1.2899069 , -0.66134655, -0.67867076,  0.10203059, -0.9303214 ],\n",
      "      dtype=float32), 'datasets': array([ 0.9742724 , -0.3071347 , -0.07830367, -0.11742125, -1.7538986 ,\n",
      "        1.4055357 ,  2.0226598 ,  0.3720822 ,  0.09516633,  1.2406613 ],\n",
      "      dtype=float32), 'trained': array([ 0.67964345,  0.19629343, -0.21653377,  0.810428  , -0.27061015,\n",
      "       -0.55447805, -0.4369749 , -0.9104887 ,  1.1097537 , -0.22040594],\n",
      "      dtype=float32), 'occurrences': array([-0.7994101 , -1.6311361 , -0.43753257, -0.58920103,  0.723988  ,\n",
      "       -0.52940667, -1.464071  , -0.3716955 , -0.63374436,  1.3427664 ],\n",
      "      dtype=float32), 'highquality': array([ 0.6325489 , -1.0273923 ,  0.7014522 , -1.1738971 , -1.1871095 ,\n",
      "        0.22367749,  0.22015572,  1.3256575 , -0.1325951 , -1.2213004 ],\n",
      "      dtype=float32), 'can': array([ 0.6492981 , -0.7482992 ,  0.11295293, -0.6764463 ,  0.21241373,\n",
      "       -0.00324744,  1.3161652 ,  0.78670865,  1.005083  ,  1.2358692 ],\n",
      "      dtype=float32), 'flexibility': array([ 1.4801193 ,  0.3605715 ,  0.3714509 , -0.16193466,  0.10265034,\n",
      "       -0.54871273,  0.9379558 ,  1.6090689 , -0.09402014, -2.3016    ],\n",
      "      dtype=float32), 'associations': array([-1.4775965 , -0.17635275,  0.07743639,  1.6784251 ,  1.2340088 ,\n",
      "       -1.0547067 ,  0.80138934,  0.62950504, -0.03413539, -1.3677505 ],\n",
      "      dtype=float32), 'such': array([-0.78039354, -0.18629044, -0.36079592,  0.9628403 , -0.05481011,\n",
      "        1.6581994 ,  0.28977332, -0.02590597, -0.10812793,  0.08906773],\n",
      "      dtype=float32), 'a': array([ 1.7568268 , -0.13227834,  0.1938595 ,  1.2484145 , -1.3002032 ,\n",
      "       -0.53044546,  0.262551  ,  0.03132778, -1.6472039 , -0.2412858 ],\n",
      "      dtype=float32), 'this': array([-0.63113433,  0.64405906,  1.1843312 ,  1.5412794 , -1.0034034 ,\n",
      "        1.4387891 ,  0.46681592, -0.47049654, -0.65638417, -1.0113266 ],\n",
      "      dtype=float32), 'words': array([-1.9294437 , -1.4191544 ,  0.96303165, -2.4412591 , -2.1543655 ,\n",
      "       -1.388421  ,  1.2709632 , -2.020437  , -0.94086623, -0.2914457 ],\n",
      "      dtype=float32), 'and': array([ 0.1213617 ,  1.4128736 ,  0.59460324,  0.4438259 , -0.5187761 ,\n",
      "        0.8934337 ,  1.6221533 ,  0.3965661 ,  0.21523768,  0.6904736 ],\n",
      "      dtype=float32), 'process': array([-0.00307008, -1.255482  , -1.6124341 , -1.0471181 ,  1.0974176 ,\n",
      "       -0.9092725 , -0.48287284, -1.5347923 ,  0.4530481 ,  0.35214943],\n",
      "      dtype=float32), 'performs': array([-0.8143387 , -0.96764755, -0.08694158,  0.9524826 , -0.60217327,\n",
      "        0.23637503, -0.1384413 ,  0.2572319 ,  1.8073363 , -0.29984412],\n",
      "      dtype=float32), 'rare': array([-0.61526144, -1.4921417 , -1.2293959 ,  1.5580678 , -0.08291065,\n",
      "        1.2156187 ,  0.7707078 , -1.2817849 , -1.0072705 , -0.13424592],\n",
      "      dtype=float32), 'resulting': array([ 3.2334445 ,  0.14459398, -0.72275835,  2.627636  ,  0.34607452,\n",
      "        1.2308564 ,  1.518856  ,  0.71318   , -1.6712592 , -1.4203894 ],\n",
      "      dtype=float32), 'meanings': array([ 0.01676401, -0.16640943,  0.16880146, -1.124429  , -1.3952821 ,\n",
      "        1.2355494 , -0.29125273,  0.08376013, -0.395698  ,  1.1543095 ],\n",
      "      dtype=float32), 'local': array([-0.5561486 ,  2.0049143 , -0.3138694 , -0.6776063 , -0.6087464 ,\n",
      "        0.75389576, -0.11494164,  1.1097382 ,  0.32525668,  0.42803866],\n",
      "      dtype=float32), 'simplicity': array([ 1.8416824 , -1.1765655 , -0.0093972 ,  0.04964413, -1.2525319 ,\n",
      "        0.01336253, -0.08960921, -1.1766789 , -1.9569304 ,  0.05705941],\n",
      "      dtype=float32), 'with': array([-1.1597586 ,  2.2762196 , -0.7850064 ,  1.3562505 ,  0.38007206,\n",
      "       -0.5014327 , -2.586428  , -0.81945634, -0.03878483,  1.0707525 ],\n",
      "      dtype=float32), 'learns': array([-0.69949883, -1.8797382 ,  1.7565554 ,  0.24575752, -1.367114  ,\n",
      "        0.0539887 ,  0.07888234,  0.07365511, -1.2874737 , -0.79710346],\n",
      "      dtype=float32), 'be': array([-0.6557829 , -0.36655954,  0.12207784, -1.1265323 , -2.055179  ,\n",
      "       -1.0603774 ,  0.45817414,  0.40323526,  1.1652985 , -0.64415336],\n",
      "      dtype=float32), 'meaningful': array([ 1.793938  ,  0.17996374, -0.11628892,  0.02006445,  0.8023027 ,\n",
      "       -0.04230277, -1.4659046 , -0.14010656, -1.5351853 ,  1.0428618 ],\n",
      "      dtype=float32), 'captures': array([-0.13775279,  1.2713281 , -0.8952293 , -2.4107542 , -0.6635473 ,\n",
      "       -1.2138762 , -0.5603312 , -0.7772143 ,  0.7874206 , -0.01512742],\n",
      "      dtype=float32), 'calculations': array([ 1.0370725 , -1.5911237 , -0.8223171 , -1.2232857 ,  0.35108945,\n",
      "       -1.1145914 ,  0.14090249,  1.6784524 , -1.1891798 ,  0.73777395],\n",
      "      dtype=float32), 'parallelization': array([ 0.408217  ,  0.29337102,  1.0813822 ,  0.51776266,  0.589316  ,\n",
      "       -2.04336   , -0.05947639, -1.5857098 ,  1.3948445 ,  1.4680965 ],\n",
      "      dtype=float32), 'window': array([-1.4621843 ,  0.39600116, -0.87598026,  0.5306422 ,  1.6975478 ,\n",
      "       -2.381835  ,  0.06561452, -0.6136565 , -0.4977462 ,  0.7637652 ],\n",
      "      dtype=float32), 'representations': array([-0.94776297, -1.324576  ,  0.61701065,  0.4119204 ,  2.5318043 ,\n",
      "       -1.791308  , -0.9774141 ,  0.6572047 ,  1.2993785 ,  1.131333  ],\n",
      "      dtype=float32), 'model': array([ 1.7122128 ,  0.87569684, -0.2078791 , -0.33481672, -0.8278991 ,\n",
      "        2.3396502 , -0.10265105,  0.5554344 , -0.36052448, -0.5411168 ],\n",
      "      dtype=float32), 'allowing': array([ 1.6128262 ,  1.0391636 , -0.78463316, -1.4429839 ,  1.5083199 ,\n",
      "        0.17097667, -0.21772224, -0.3088733 , -2.7652965 , -1.0628502 ],\n",
      "      dtype=float32), 'each': array([-1.3534766 , -1.2789176 , -0.6045884 ,  1.2465991 ,  1.1648202 ,\n",
      "       -0.18392473,  0.03730816,  0.45374623, -0.98864716,  1.116159  ],\n",
      "      dtype=float32), 'contextual': array([ 2.107982  , -1.5831324 , -0.8517765 ,  0.9204345 , -0.13295354,\n",
      "        0.16704048,  1.1664397 , -0.14457713,  1.5169505 , -0.5129461 ],\n",
      "      dtype=float32), 'appear': array([-0.8130018 ,  0.35905978,  0.04153208,  0.0123555 ,  0.738423  ,\n",
      "        0.2405012 ,  0.2979473 ,  3.0617533 , -0.51571107, -0.77742845],\n",
      "      dtype=float32), 'even': array([ 0.32192573,  1.9975536 , -0.60719067, -1.8111328 , -1.207329  ,\n",
      "       -0.16900566, -2.1400964 , -0.04755113,  1.2475629 , -1.4745133 ],\n",
      "      dtype=float32), 'allows': array([ 1.3315687 ,  0.12933114, -0.24422562, -0.9943093 ,  0.13632636,\n",
      "        1.503976  , -0.7082896 , -0.3948764 , -0.63604635, -0.03115765],\n",
      "      dtype=float32), 'skipgram': array([-2.0282743 ,  1.1739439 , -0.7461346 , -0.5332132 , -1.8082279 ,\n",
      "        2.41246   , -0.08737875,  0.5046098 ,  0.5033901 ,  0.1659183 ],\n",
      "      dtype=float32), 'it': array([ 0.8313854 ,  0.2820509 , -0.43407127,  0.6470958 , -1.80603   ,\n",
      "       -0.02751438,  0.44550604,  1.3363562 ,  1.9594898 ,  1.7964717 ],\n",
      "      dtype=float32), 'scalability': array([ 1.2618823 , -0.29477137,  1.243145  ,  0.7763355 , -0.5395161 ,\n",
      "       -0.28120267, -0.83989143, -0.6803521 ,  0.7681507 ,  0.08667419],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Функция для создания словаря и подготовки данных\n",
    "def prepare_data_skip_gram(text: str, window_size=2):\n",
    "\t# Удаляем все символы кроме a-z, @, и #\n",
    "\ttext = re.sub(r'[^a-z@# ]', '', text.lower())    \n",
    "\t# Разбиваем на пробелы\n",
    "\ttokens = text.split()    \n",
    "\t\n",
    "\tvocab = set(tokens)\n",
    "\tword_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\t\n",
    "\tdata = []\n",
    "\tfor i in range(len(tokens)):\n",
    "\t\tfor j in range(max(0, i - window_size), min(len(tokens), i + window_size + 1)):\n",
    "\t\t\tif i != j:\n",
    "\t\t\t\tdata.append((tokens[i], tokens[j]))\t\n",
    "\treturn data, word_to_ix, len(vocab)\t\n",
    " \n",
    "class SkipGramDataset(Dataset):\n",
    "\tdef __init__(self, data, word_to_ix):\t\t\t\n",
    "\t\tself.data = [(word_to_ix[center], word_to_ix[context]) for center, context in data]\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn torch.tensor(self.data[idx][0], dtype=torch.long), torch.tensor(self.data[idx][1], dtype=torch.long)\n",
    "\t\n",
    "\n",
    "class Word2VecSkipGramModel(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embedding_dim):\n",
    "\t\tsuper(Word2VecSkipGramModel, self).__init__()\n",
    "\t\tself.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\t\tself.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\t\tself.activation_function = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "\tdef forward(self, center_word_idx):\n",
    "\t\thidden_layer = self.embeddings(center_word_idx)\n",
    "\t\tout_layer = self.out_layer(hidden_layer)\n",
    "\t\tlog_probs = self.activation_function(out_layer)\n",
    "\t\treturn log_probs\n",
    "\n",
    "# Функция обучения модели\n",
    "def train_model(data, word_to_ix, vocab_size, embedding_dim=50, epochs=10, batch_size=1):\n",
    "\tdataset = SkipGramDataset(data, word_to_ix)\n",
    "\tdataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\t\n",
    "\tmodel = Word2VecSkipGramModel(vocab_size, embedding_dim)\n",
    "\tloss_function = nn.NLLLoss()\n",
    "\toptimizer = torch.optim.SGD (model.parameters(), lr=0.05)\n",
    "\tprint('start')\n",
    "\tfor epoch in range(epochs):\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor center_word, context_word in dataloader:\n",
    "\t\t\tmodel.zero_grad()\n",
    "\t\t\tlog_probs = model(center_word)\n",
    "\t\t\tloss = loss_function(log_probs, context_word)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()            \n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\t\t\n",
    "\t\tprint(f'Epoch {epoch + 1}, Loss: {total_loss}')\n",
    "\treturn model\n",
    "\n",
    "# Главная функция\n",
    "def train(data: str):\n",
    "\twindow_size = 2\n",
    "\tembedding_dim = 10\n",
    "\tepochs = 5\n",
    "\tbatch_size = 2\n",
    "\t\n",
    "\tngramm_data, word_to_ix, vocab_size = prepare_data_skip_gram(data, window_size)    \n",
    "\tmodel = train_model(ngramm_data, word_to_ix, vocab_size, embedding_dim, epochs, batch_size)\n",
    "\t\n",
    "\t# # Извлекаем векторы слов из модели\n",
    "\tembeddings = model.embeddings.weight.data.numpy()\n",
    "\tix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "\tw2v_dict = {ix_to_word[ix]: embeddings[ix] for ix in range(vocab_size)}\n",
    "\treturn w2v_dict\n",
    "# Тестовые данные\n",
    "test_text = 'Captures Semantic Relationships: The skip-gram model effectively captures semantic relationships between words. It learns word embeddings that encode similar meanings and associations, allowing for tasks like word analogies and similarity calculations. Handles Rare Words: The skip-gram model performs well even with rare words or words with limited occurrences in the training data. It can generate meaningful representations for such words by leveraging the context in which they appear. Contextual Flexibility: The skip-gram model allows for flexible context definitions by using a window around each target word. This flexibility captures local and global word associations, resulting in richer semantic representations. Scalability: The skip-gram model can be trained efficiently on large-scale datasets due to its simplicity and parallelization potential. It can process vast amounts of text data to generate high-quality word embeddings.'\n",
    "\n",
    "w2v_dict = train(test_text)\n",
    "print(w2v_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 280.15081763267517\n",
      "Epoch 2, Loss: 280.03195548057556\n",
      "Epoch 3, Loss: 279.91373205184937\n",
      "Epoch 4, Loss: 279.79516768455505\n",
      "Epoch 5, Loss: 279.67906975746155\n",
      "{'handles': array([-0.07003074, -0.6746009 ,  0.21859492, -2.5551062 , -0.22458036,\n",
      "       -1.7734305 , -0.07510825, -0.5323997 ,  0.8523845 ,  0.24517506],\n",
      "      dtype=float32), 'vast': array([-0.23910834, -0.7070684 , -1.4162719 ,  0.4358957 , -0.32772398,\n",
      "       -0.72569036, -0.22478838,  1.3738844 , -0.63622606,  0.55530167],\n",
      "      dtype=float32), 'efficiently': array([-0.09773862, -2.3268185 ,  0.2261297 , -0.87509406, -0.5048835 ,\n",
      "        0.590961  , -0.51213163,  0.67964023,  0.36683547, -0.35292512],\n",
      "      dtype=float32), 'semantic': array([ 0.30830273,  0.09844723,  1.3344359 ,  1.7950017 ,  1.0287782 ,\n",
      "        1.7078141 , -0.04264637, -1.4423107 ,  0.13102528,  1.3155242 ],\n",
      "      dtype=float32), 'well': array([ 0.35462442, -0.6673486 , -0.48746923, -0.80354077,  1.6111429 ,\n",
      "       -0.4167712 ,  0.3698388 , -0.23118089, -0.5765213 , -0.71563697],\n",
      "      dtype=float32), 'to': array([ 0.99596244, -0.6410427 , -0.5234671 , -1.3401594 ,  1.0600536 ,\n",
      "       -0.6073951 ,  1.3967812 ,  1.580847  , -2.3503706 ,  0.03548317],\n",
      "      dtype=float32), 'its': array([-0.47451055,  1.1650447 , -1.4644959 ,  0.32400006, -1.8995912 ,\n",
      "        0.14953014,  0.87824774, -0.5158742 ,  0.7699255 , -0.9378788 ],\n",
      "      dtype=float32), 'effectively': array([-1.6628842 ,  0.6729694 ,  0.87686056, -0.8203202 , -0.5770072 ,\n",
      "       -0.08325996,  0.72651637, -0.01515894,  0.25115734, -0.6484685 ],\n",
      "      dtype=float32), 'amounts': array([-0.14484406, -0.9063926 ,  2.008324  , -0.16444303, -1.6752933 ,\n",
      "       -0.58093756, -0.6985722 , -0.27971852,  1.7048645 ,  0.489869  ],\n",
      "      dtype=float32), 'encode': array([-0.90064317, -0.571076  , -0.03522158,  0.98087734, -0.42551652,\n",
      "       -1.4795953 , -0.10847533, -1.6358489 ,  1.0224965 ,  0.25406152],\n",
      "      dtype=float32), 'analogies': array([ 0.71125454, -1.4390588 , -0.74711186,  1.8709239 , -0.64141494,\n",
      "       -1.1558244 ,  0.04717525,  0.36316603,  1.0280049 , -1.069536  ],\n",
      "      dtype=float32), 'the': array([-0.4897755 ,  0.60730237,  1.4857165 , -1.3474408 , -0.25224528,\n",
      "        0.86166364,  0.26369077,  0.17565948, -0.00323787, -1.0397753 ],\n",
      "      dtype=float32), 'potential': array([ 0.4567806 ,  0.8001634 , -0.4213148 , -0.19426739,  0.93018764,\n",
      "       -0.16593297, -0.11872242, -0.75666326, -1.571742  ,  1.2698667 ],\n",
      "      dtype=float32), 'tasks': array([ 0.54434067,  0.98549354,  0.35257018,  0.6197899 ,  0.6702074 ,\n",
      "        0.5927019 , -1.080624  ,  1.3857499 , -1.4145732 ,  0.17362905],\n",
      "      dtype=float32), 'limited': array([ 1.3202169 ,  0.27202293, -0.9687948 ,  0.14537638, -0.7834612 ,\n",
      "       -0.77880055,  0.6821279 , -0.4617927 ,  0.02469347,  0.12094229],\n",
      "      dtype=float32), 'relationships': array([ 0.30000198, -1.6854904 , -1.4352808 ,  0.7845225 , -0.05684105,\n",
      "        0.68155116, -1.3325446 , -1.3522396 ,  0.3370798 , -0.7807204 ],\n",
      "      dtype=float32), 'in': array([ 1.767667  , -0.1723032 ,  0.19934517, -0.787732  , -1.3890177 ,\n",
      "        0.790569  ,  0.9077234 ,  0.26306406, -0.28200898,  0.6298198 ],\n",
      "      dtype=float32), 'that': array([ 0.69677275, -0.5974694 , -1.0083902 ,  0.53381354,  0.7963123 ,\n",
      "        0.09890386,  1.3152463 , -2.4463387 ,  1.4434912 ,  1.2668108 ],\n",
      "      dtype=float32), 'on': array([ 0.17219475, -0.46806428, -1.4600507 ,  1.0477351 , -0.96952116,\n",
      "        1.0282344 ,  0.93714666, -1.8737056 ,  0.7168962 ,  0.5495494 ],\n",
      "      dtype=float32), 'flexible': array([ 0.43259484, -0.3255251 ,  2.3802738 , -0.08646791,  0.84997416,\n",
      "        0.55636775,  0.30771074,  0.14397281, -1.1919075 , -1.4816464 ],\n",
      "      dtype=float32), 'which': array([-0.8574073 , -0.98565817, -0.25613922,  1.1463546 , -0.3119282 ,\n",
      "        0.5311311 ,  0.00425059,  0.42242277, -0.06421215,  0.6521491 ],\n",
      "      dtype=float32), 'generate': array([-0.13575777,  1.016854  ,  0.4578514 ,  0.5061097 ,  0.7059765 ,\n",
      "        0.86882126,  1.686365  , -1.029066  ,  0.44947872,  0.21957058],\n",
      "      dtype=float32), 'like': array([-1.6276076 ,  0.06156989,  1.2913574 , -0.23433015,  0.03992124,\n",
      "        0.74171525, -0.6277932 ,  0.5105191 ,  0.08230783, -1.135778  ],\n",
      "      dtype=float32), 'by': array([-0.56285363, -0.6000301 , -0.40303704, -0.8578126 , -0.5062121 ,\n",
      "       -1.2408683 ,  1.6091161 ,  0.6843925 , -0.84540087, -0.02436288],\n",
      "      dtype=float32), 'or': array([ 0.86879146, -0.40768683, -0.23827487, -0.37327132,  0.05965161,\n",
      "        1.6579459 ,  0.03579922,  0.19740732, -0.1918671 , -0.3194878 ],\n",
      "      dtype=float32), 'data': array([ 0.43465957, -0.6990835 , -0.44179294,  2.504163  ,  0.3089728 ,\n",
      "        0.58980876,  1.1582942 , -0.666058  ,  1.0424525 , -0.5748226 ],\n",
      "      dtype=float32), 'around': array([ 0.9708973 ,  1.32549   , -0.1655268 ,  0.2248274 , -0.40775618,\n",
      "       -0.08391269, -0.09862409, -0.9338675 ,  0.73572606,  0.36487868],\n",
      "      dtype=float32), 'for': array([ 1.5302883 , -0.9639332 ,  0.27939066, -0.77731806, -0.994333  ,\n",
      "       -0.9847944 ,  1.5515639 , -0.7539149 , -0.2278323 ,  0.7618587 ],\n",
      "      dtype=float32), 'similar': array([-0.04736441,  0.16137972,  0.40143865,  1.1894767 ,  0.36851496,\n",
      "        1.8397623 , -1.2964239 , -0.19380225, -1.2501277 , -1.9529531 ],\n",
      "      dtype=float32), 'target': array([-1.2265147 ,  0.52406204,  0.1037685 , -0.4144702 , -0.18145125,\n",
      "        0.40786222, -0.4678443 ,  0.18455294,  0.14506628,  0.44519302],\n",
      "      dtype=float32), 'leveraging': array([-0.6722094 , -0.3150657 ,  0.06419247,  2.5048754 ,  0.50899535,\n",
      "        0.5745923 , -0.85968405,  0.13348965, -0.7358494 , -0.84655035],\n",
      "      dtype=float32), 'of': array([ 0.47835517,  0.5682748 ,  0.6462122 ,  0.43973094,  0.5198112 ,\n",
      "       -0.33358955, -2.3495553 ,  0.09386072,  0.84983855, -0.5334607 ],\n",
      "      dtype=float32), 'global': array([ 2.123109  ,  1.8922659 , -0.4575133 ,  0.9727992 , -1.5125338 ,\n",
      "       -0.48882455,  0.7678582 , -0.79598266,  1.0713632 ,  0.08595732],\n",
      "      dtype=float32), 'definitions': array([-0.932651  , -0.4955801 , -0.03663978,  1.7295109 ,  0.4564976 ,\n",
      "       -0.46174932, -0.30362904,  2.2416    ,  0.32284206, -1.9126704 ],\n",
      "      dtype=float32), 'between': array([ 1.5000587 , -0.9178688 , -0.7081788 , -0.31198457, -0.0464954 ,\n",
      "        1.1015214 ,  0.14307289,  0.22183932,  0.27776957,  0.6117857 ],\n",
      "      dtype=float32), 'due': array([-0.8006584 ,  0.6417908 ,  2.0886283 , -1.6297137 ,  0.06968248,\n",
      "       -0.04209073,  0.8375391 , -0.10961071, -0.3662741 , -0.28382266],\n",
      "      dtype=float32), 'word': array([ 2.0535264 , -0.6803512 ,  1.4817092 ,  1.0793993 , -0.03394295,\n",
      "       -1.371927  ,  0.04496676,  0.46166855, -1.583975  ,  0.41919807],\n",
      "      dtype=float32), 'similarity': array([ 0.36616045,  0.75152385, -1.9990135 , -0.48491502,  0.4191355 ,\n",
      "        0.52615047,  0.58063906, -0.9976319 ,  0.15069441,  0.01856804],\n",
      "      dtype=float32), 'text': array([-0.07496504, -0.16709228, -0.2840653 , -1.1911453 ,  0.6638108 ,\n",
      "       -0.6382396 ,  0.6685629 , -0.87476474, -1.0110025 ,  1.2763511 ],\n",
      "      dtype=float32), 'training': array([ 0.23247592,  0.21852115, -0.01090498,  0.08265553,  0.43343443,\n",
      "       -1.2409356 ,  0.5767261 ,  0.6877892 ,  1.4640594 , -0.9160476 ],\n",
      "      dtype=float32), 'using': array([ 0.29267526, -0.6516279 , -1.1607605 , -1.8528422 ,  2.6100276 ,\n",
      "       -0.41830793,  0.8276998 ,  0.73678017, -0.21750103, -1.2597686 ],\n",
      "      dtype=float32), 'largescale': array([ 0.74763733,  1.0940983 , -0.8161141 ,  0.24367575,  1.1737163 ,\n",
      "        0.6299729 ,  0.3794938 , -0.44334698, -0.0630429 , -0.67461056],\n",
      "      dtype=float32), 'richer': array([ 0.9109185 ,  0.15339783,  0.15836914,  0.5660744 ,  0.30241427,\n",
      "        1.2129891 ,  0.9897319 ,  1.5115281 , -0.4377949 , -1.1470478 ],\n",
      "      dtype=float32), 'embeddings': array([ 1.6568694 ,  0.7867111 , -0.3203124 ,  0.16655736,  0.10238319,\n",
      "        0.80723524, -0.30648905, -1.9356918 , -0.06087507, -0.25987285],\n",
      "      dtype=float32), 'context': array([-1.1545757 ,  0.29526246,  0.02139639, -0.59054697,  0.3421384 ,\n",
      "        2.1934679 ,  1.9245847 , -0.8248302 , -0.8441554 ,  0.64662814],\n",
      "      dtype=float32), 'they': array([ 0.640582  , -0.5075108 , -1.0390517 ,  0.06522784, -0.5784596 ,\n",
      "       -0.33968842, -1.7439567 ,  0.71937877,  0.7866579 ,  0.4539833 ],\n",
      "      dtype=float32), 'datasets': array([ 0.00563243,  0.15784952,  0.63196087, -1.4074333 , -1.4924554 ,\n",
      "        0.13524517,  1.4279648 , -1.1664734 ,  1.077538  , -0.21436222],\n",
      "      dtype=float32), 'trained': array([-1.3550023 ,  0.6912217 ,  0.0777758 ,  0.7235072 , -1.3832619 ,\n",
      "        0.51331663, -1.7418718 ,  1.5173111 ,  0.7727141 ,  0.40575677],\n",
      "      dtype=float32), 'occurrences': array([ 1.913244  , -2.0062692 ,  0.44442275,  1.1069696 ,  0.21668242,\n",
      "        0.09658352,  2.0965986 ,  1.7820461 , -2.8302033 , -0.5107815 ],\n",
      "      dtype=float32), 'highquality': array([ 1.1982415 , -0.16928735, -1.153047  ,  3.2202559 , -0.62659055,\n",
      "        0.9476903 ,  1.2710446 ,  0.56541413, -0.20174947, -0.14466894],\n",
      "      dtype=float32), 'can': array([-0.33308828,  0.5114868 ,  1.0754973 ,  0.57458484, -0.5202784 ,\n",
      "        0.5738604 , -0.1235254 ,  1.3075849 , -0.45160526,  1.088122  ],\n",
      "      dtype=float32), 'flexibility': array([-1.5413246 , -1.9289899 , -1.5301032 ,  0.75645375, -0.07632833,\n",
      "       -2.0344617 ,  0.9696219 ,  0.6460726 ,  1.13977   , -0.02062094],\n",
      "      dtype=float32), 'associations': array([-0.00967224, -0.5392614 , -0.50290126,  0.17261715,  0.77153724,\n",
      "        0.77168995,  1.3558538 , -0.06436366, -0.48348388,  2.166248  ],\n",
      "      dtype=float32), 'such': array([ 0.619172  , -0.3756253 ,  1.4722271 ,  0.18467328,  0.40360883,\n",
      "        1.6287266 ,  0.26044735, -0.44394922, -0.15195543,  1.1126033 ],\n",
      "      dtype=float32), 'a': array([-0.242058  , -1.0125958 ,  1.7326995 ,  2.206386  ,  0.00452989,\n",
      "       -1.0365785 ,  0.5209485 ,  0.03097812,  0.17092068,  0.32285976],\n",
      "      dtype=float32), 'this': array([-1.2433007 ,  0.74420214, -0.52549   , -0.61869466,  0.15241885,\n",
      "       -0.78887486,  0.08282446,  0.02347158, -0.19387652,  0.7693193 ],\n",
      "      dtype=float32), 'words': array([-1.9327471 ,  0.4062305 ,  1.147984  ,  1.102758  ,  0.5734437 ,\n",
      "        1.2467611 , -1.1680894 ,  0.87719136,  1.3729829 ,  0.44984186],\n",
      "      dtype=float32), 'and': array([-0.34352702, -0.77068657, -2.283837  , -0.9767043 ,  0.9702759 ,\n",
      "        0.28693402,  0.32542244, -2.4796557 ,  0.12830044, -0.9666863 ],\n",
      "      dtype=float32), 'process': array([ 0.49239364, -1.3253872 , -0.07962324, -0.07561559,  0.15923147,\n",
      "        0.94665545,  0.11589307,  0.58694655, -0.18163618,  1.7922165 ],\n",
      "      dtype=float32), 'performs': array([-1.6084939 , -0.06716985,  1.1667045 , -1.2060543 , -0.99852747,\n",
      "       -0.9497376 , -0.48916617, -1.6787968 ,  0.07253063, -0.9608168 ],\n",
      "      dtype=float32), 'rare': array([ 0.7956735 , -1.5231789 , -0.19550563,  0.56338865,  1.9379276 ,\n",
      "       -0.4639776 ,  0.32167193,  0.5040505 ,  0.6284498 , -0.5702444 ],\n",
      "      dtype=float32), 'resulting': array([ 0.8440293 , -0.8526878 ,  0.08922459,  1.0277969 , -1.0797274 ,\n",
      "       -0.22167504,  0.4061992 , -0.18827012, -0.38172585,  0.9559581 ],\n",
      "      dtype=float32), 'meanings': array([-1.4442996 , -0.5808866 , -1.0192621 ,  0.89285123,  1.5755128 ,\n",
      "       -0.6413497 ,  0.02952979, -2.092786  ,  0.5763861 ,  1.6425009 ],\n",
      "      dtype=float32), 'local': array([ 0.96120334, -0.06830983,  0.71289295, -0.11511946, -1.7582859 ,\n",
      "        0.22928804, -0.59668964,  0.05117374,  0.43500453,  1.0247737 ],\n",
      "      dtype=float32), 'simplicity': array([ 0.50827473,  1.5095102 , -1.2934783 , -0.13521276,  0.37725255,\n",
      "        1.0010377 ,  0.3593552 ,  0.2141469 ,  0.4581213 ,  1.0579895 ],\n",
      "      dtype=float32), 'with': array([ 0.01398302,  1.6780717 , -0.6302911 , -0.25401568,  0.88322866,\n",
      "       -0.34885535, -0.14510646, -0.15439932,  0.81360656,  0.68084496],\n",
      "      dtype=float32), 'learns': array([ 1.2598777 ,  0.4557412 , -0.01331883,  0.7380728 , -1.5271497 ,\n",
      "        0.21328348, -0.74824   , -0.19252051, -0.614189  , -0.21552248],\n",
      "      dtype=float32), 'be': array([-0.2406088 ,  0.47485486, -1.1770157 , -0.22558987, -1.379694  ,\n",
      "       -0.6386101 ,  0.44442585,  0.97929776,  0.81180286, -0.16492742],\n",
      "      dtype=float32), 'meaningful': array([-0.6757202 ,  2.4169824 ,  0.7606467 , -0.10524616,  0.05696829,\n",
      "        1.0512823 , -0.46158412,  0.31436825, -0.8066354 , -1.9812396 ],\n",
      "      dtype=float32), 'captures': array([ 2.6655464 ,  2.1026802 , -0.84484273,  0.34505108, -1.7884676 ,\n",
      "        0.58111775, -0.60123426, -0.9245146 ,  0.80760616,  0.61024976],\n",
      "      dtype=float32), 'calculations': array([ 0.71808344, -0.5462488 , -0.3202869 , -0.48395264, -0.08304832,\n",
      "       -0.05841504, -0.20883597,  1.0194869 , -0.29773134, -1.6186037 ],\n",
      "      dtype=float32), 'parallelization': array([ 0.05830023,  0.9297203 ,  0.44905445,  1.2809914 ,  0.06081741,\n",
      "        1.3101615 ,  0.74733484,  0.9362394 , -0.52776396, -1.2643265 ],\n",
      "      dtype=float32), 'window': array([ 1.8186584 ,  0.8099673 ,  1.9841303 , -0.36875162,  1.6845996 ,\n",
      "       -1.0934156 ,  1.9265636 ,  0.28087774, -0.41482377, -0.92304474],\n",
      "      dtype=float32), 'representations': array([-1.2182552 ,  0.02652281,  0.8088531 ,  0.2381808 , -0.48844573,\n",
      "       -0.9572753 , -0.09012445,  0.4684028 ,  0.00971961,  0.9225686 ],\n",
      "      dtype=float32), 'model': array([ 1.0879838 , -0.80298656,  0.5052225 , -0.07742021, -0.3025239 ,\n",
      "       -0.53853816,  1.4656978 , -0.71613115, -0.9563203 ,  1.3589696 ],\n",
      "      dtype=float32), 'allowing': array([ 1.8598139 ,  1.0650592 , -1.1726236 , -2.1360452 ,  0.72254604,\n",
      "       -0.0384877 , -0.6204267 ,  1.3249264 ,  1.5508746 ,  0.35198572],\n",
      "      dtype=float32), 'each': array([ 0.36506432,  0.16491145,  0.07952451, -0.9852999 ,  0.8813371 ,\n",
      "        0.3441203 , -1.611455  ,  0.46002045, -1.7411821 ,  1.3452433 ],\n",
      "      dtype=float32), 'contextual': array([-1.2101871 , -0.24412759, -0.2403081 , -0.12593433, -0.35087156,\n",
      "       -1.1481718 ,  1.9499682 ,  0.84993476, -0.12375332,  1.6376519 ],\n",
      "      dtype=float32), 'appear': array([-0.07382363,  1.8365686 , -1.9295222 , -0.05348536,  1.1413785 ,\n",
      "       -0.11785956,  2.1348991 , -0.29695466,  0.31539857,  1.5252653 ],\n",
      "      dtype=float32), 'even': array([ 0.9421886 ,  1.5592213 ,  0.59745383, -0.15912217, -0.8122096 ,\n",
      "        2.665692  , -0.40702647,  0.8636119 ,  1.138295  , -0.5969641 ],\n",
      "      dtype=float32), 'allows': array([-0.2927193 , -0.37857476, -1.702246  , -0.56238806,  0.15854183,\n",
      "        0.90986854, -0.4741676 , -0.4108126 , -1.4428599 , -0.0677611 ],\n",
      "      dtype=float32), 'skipgram': array([-0.07908593, -0.10714693,  0.80642843, -0.4744827 ,  0.33700642,\n",
      "       -1.592126  ,  2.4854615 , -0.2529006 , -0.06204023,  1.6241148 ],\n",
      "      dtype=float32), 'it': array([-1.108487  ,  0.11254408,  0.3401441 ,  1.8576367 ,  0.9699543 ,\n",
      "       -0.37931836, -0.03312515,  0.5075577 ,  0.76746565, -1.7728004 ],\n",
      "      dtype=float32), 'scalability': array([-1.2426057 , -0.2467624 ,  0.78716874,  0.07030875, -0.21781652,\n",
      "        1.7072644 ,  1.1787271 ,  0.02838622,  1.2148995 ,  0.23097137],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def prepare_data_cbow(text: str, window_size=2):\n",
    "\ttext = re.sub(r'[^a-z@# ]', '', text.lower())    \n",
    "\ttokens = text.split()    \n",
    "\t\n",
    "\tvocab = set(tokens)\n",
    "\tword_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\t\n",
    "\tdata = []\n",
    "\tfor i in range(window_size, len(tokens) - window_size):\n",
    "\t\tcontext = [tokens[i - j - 1] for j in range(window_size)] + [tokens[i + j + 1] for j in range(window_size)]\n",
    "\t\ttarget = tokens[i]\n",
    "\t\tdata.append((context, target))\n",
    "\treturn data, word_to_ix, len(vocab)\t\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "\tdef __init__(self, data, word_to_ix):\n",
    "\t\tself.contexts = []\n",
    "\t\tself.targets = []\n",
    "\t\tfor context, target in data:\n",
    "\t\t\tindexed_context = [word_to_ix[word] for word in context]\n",
    "\t\t\tself.contexts.append(indexed_context)\n",
    "\t\t\tself.targets.append(word_to_ix[target])\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.targets)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t# Возвращаем контекст и центральное слово как пару тензоров\n",
    "\t\treturn torch.tensor(self.contexts[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "class Word2VecCBOWModel(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embedding_dim):\n",
    "\t\tsuper(Word2VecCBOWModel, self).__init__()\n",
    "\t\tself.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\t\tself.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\t\tself.activation_function = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\tdef forward(self, center_word_idx):\n",
    "\t\thidden_layer = torch.mean(self.embeddings(center_word_idx), dim=1)\n",
    "\t\tout_layer = self.out_layer(hidden_layer)\n",
    "\t\tlog_probs = self.activation_function(out_layer)\n",
    "\t\treturn log_probs\n",
    "\n",
    "# Функция обучения модели\n",
    "def train_model_cbow(data, word_to_ix, vocab_size, embedding_dim=50, epochs=10, batch_size=1):\n",
    "\tdataset = CBOWDataset(data, word_to_ix)\n",
    "\tdataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\t\n",
    "\tmodel = Word2VecCBOWModel(vocab_size, embedding_dim)\n",
    "\tloss_function = nn.NLLLoss()\n",
    "\toptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\t\n",
    "\tfor epoch in range(epochs):\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor context_words, target_word in dataloader:\n",
    "\t\t\tcontext_words = context_words  # Подготавливаем контекстные слова\n",
    "\t\t\tmodel.zero_grad()\n",
    "\t\t\tlog_probs = model(context_words)\n",
    "\t\t\tloss = loss_function(log_probs, target_word)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\tprint(f'Epoch {epoch + 1}, Loss: {total_loss}')\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# Главная функция\n",
    "def train(data: str):\n",
    "\twindow_size = 2\n",
    "\tembedding_dim = 10\n",
    "\tepochs = 5\n",
    "\tbatch_size = 2\n",
    "\t\n",
    "\tngramm_data, word_to_ix, vocab_size = prepare_data_cbow(data, window_size)    \n",
    "\tmodel = train_model_cbow(ngramm_data, word_to_ix, vocab_size, embedding_dim, epochs, batch_size)\n",
    "\t\n",
    "\t# # Извлекаем векторы слов из модели\n",
    "\tembeddings = model.embeddings.weight.data.numpy()\n",
    "\tix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "\tw2v_dict = {ix_to_word[ix]: embeddings[ix] for ix in range(vocab_size)}\n",
    "\treturn w2v_dict\n",
    "# Тестовые данные\n",
    "test_text = 'Captures Semantic Relationships: The skip-gram model effectively captures semantic relationships between words. It learns word embeddings that encode similar meanings and associations, allowing for tasks like word analogies and similarity calculations. Handles Rare Words: The skip-gram model performs well even with rare words or words with limited occurrences in the training data. It can generate meaningful representations for such words by leveraging the context in which they appear. Contextual Flexibility: The skip-gram model allows for flexible context definitions by using a window around each target word. This flexibility captures local and global word associations, resulting in richer semantic representations. Scalability: The skip-gram model can be trained efficiently on large-scale datasets due to its simplicity and parallelization potential. It can process vast amounts of text data to generate high-quality word embeddings.'\n",
    "\n",
    "w2v_dict = train(test_text)\n",
    "print(w2v_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
